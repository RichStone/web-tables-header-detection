{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change global plot settings\n",
    "plt.rcParams.update({'font.size': 24})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelBars(plot):\n",
    "    for p in plot.patches:\n",
    "        plot.annotate(\n",
    "            np.round(p.get_height(), decimals=2),\n",
    "            (\n",
    "                p.get_x() + p.get_width() / 2.,\n",
    "                p.get_height()\n",
    "            ),\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            xytext=(0, 10),\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({\"annotatedAt\" : {\"$exists\" : True}, \"skipped\": {\"$ne\": True}})\n",
    "tables = pd.DataFrame(list(cursor))\n",
    "client.close()\n",
    "tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTables = pd.DataFrame(json.load(open(os.path.join('..', 'data', 'test_wtc.json'), 'r')).values(), columns=['_id'])\n",
    "testTables = testTables['_id'].apply(lambda tableID: tables.loc[tables['_id'].astype(str) == tableID].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTables = pd.DataFrame(json.load(open(os.path.join('..', 'data', 'train.json'), 'r')).values(), columns=['_id'])\n",
    "trainTables = trainTables['_id'].apply(lambda tableID: tables.loc[tables['_id'].astype(str) == tableID].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table count per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total tables count: ', len(tables))\n",
    "print('Training tables count: ', len(trainTables))\n",
    "print('Test tables count: ', len(testTables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg. Table size (row count) per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRowCount(tables):\n",
    "    return tables.apply(lambda table: len(table['annotations']), axis='columns')\n",
    "\n",
    "print('Avg. table size for training set: ', round(getRowCount(trainTables).mean()))\n",
    "print('Avg. table size for training set: ', round(getRowCount(testTables).mean()))\n",
    "print('Avg. table size for training set: ', round(getRowCount(tables).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset cell type ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalCounter = Counter([item for sublist in list(tables['annotations'].values) for item in sublist])\n",
    "trainCounter = Counter([item for sublist in list(trainTables['annotations'].values) for item in sublist])\n",
    "testCounter = Counter([item for sublist in list(testTables['annotations'].values) for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRatio(counter):\n",
    "    total = sum(list(counter.values()))\n",
    "    return [counter[annotation] * 100.0 / total for annotation in ['Other', 'Header', 'Data']]\n",
    "    \n",
    "ratios = [getRatio(testCounter), getRatio(trainCounter), getRatio(totalCounter)]\n",
    "ratioLabels = ['Test', 'Train', 'Total']\n",
    "ratioDf = pd.DataFrame(ratios, index=ratioLabels, columns=['Other', 'Header', 'Data']).T\n",
    "ratioPlot = ratioDf.plot(kind='barh', figsize=(20, 10))\n",
    "ratioPlot.set(\n",
    "    xlabel=\"Percentage\",\n",
    "    ylabel=\"Annotation\", \n",
    "    title=\"Distribution of cell types accross datasets\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctly predicted tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({\"predictions\" : {\"$exists\" : True}})\n",
    "predictedTables = pd.DataFrame(list(cursor))\n",
    "client.close()\n",
    "predictedTables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWronglyPredicted(table):\n",
    "    incorrect = []\n",
    "    for rowIndex, label in enumerate(table['annotations']):\n",
    "        if(label != table['predictions'][rowIndex]):\n",
    "            incorrect.append({\n",
    "                'annotated': label,\n",
    "                'predicted': table['predictions'][rowIndex]\n",
    "            })\n",
    "    return incorrect\n",
    "\n",
    "def getCorrectlyPredictedCount(table):\n",
    "    return len(table['annotations']) - table['predictedIncorrectlyCount']\n",
    "    \n",
    "def isWholeTableCorrectlyPredicted(table):\n",
    "    return table['predictedCorrectlyCount'] == len(table['annotations'])\n",
    "\n",
    "def getIncorrectlyPredictedCount(table):\n",
    "    return len(table['wronglyPredicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedTables['wronglyPredicted'] = predictedTables.apply(getWronglyPredicted, axis='columns')\n",
    "predictedTables['predictedIncorrectlyCount'] = predictedTables.apply(getIncorrectlyPredictedCount, axis='columns')\n",
    "predictedTables['predictedCorrectlyCount'] = predictedTables.apply(getCorrectlyPredictedCount, axis='columns')\n",
    "predictedTables['predictedCorrectly'] = predictedTables.apply(isWholeTableCorrectlyPredicted, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctlyPredictedTables = predictedTables.loc[predictedTables['predictedCorrectly']].shape[0]\n",
    "print('Correctly predicted table count: ' + str(correctlyPredictedTables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectPredictedTables = predictedTables.loc[~predictedTables['predictedCorrectly']]\n",
    "incorrectPredictedTables.reset_index(inplace=True)\n",
    "incorrectPredictedTableCount = incorrectPredictedTables.shape[0]\n",
    "print('Incorrect predicted table count: ' + str(incorrectPredictedTableCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctly predicted rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfIncorrectRowsPerTablePlot = incorrectPredictedTables['predictedIncorrectlyCount'].value_counts().plot(\n",
    "    kind='bar', \n",
    "    figsize=(20, 10)\n",
    ")\n",
    "labelBars(numberOfIncorrectRowsPerTablePlot)\n",
    "numberOfIncorrectRowsPerTablePlot.set(\n",
    "   xlabel='Amount of incorrectly labeled rows per table',\n",
    "    ylabel='Amount of tables',\n",
    "    title='Incorrectly labled rows per table'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRowCount(table):\n",
    "    return len(table['annotations'])\n",
    "\n",
    "rowCountPerTablePlot = incorrectPredictedTables.apply(getRowCount, axis='columns').value_counts().plot(\n",
    "    kind='bar', \n",
    "    figsize=(20, 10)\n",
    ")\n",
    "labelBars(rowCountPerTablePlot)\n",
    "rowCountPerTablePlot.set(\n",
    "   xlabel='Table size (total table row count)',\n",
    "    ylabel='Amount of tables',\n",
    "    title='Dependency between table size (total row count) and prediction correctness'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wronglyPredicted = list(incorrectPredictedTables['wronglyPredicted'])\n",
    "wronglyPredicted = [item for sublist in wronglyPredicted for item in sublist]\n",
    "wronglyPredicted = pd.DataFrame(wronglyPredicted)\n",
    "print('Count of which row type got predicted incorrectly:')\n",
    "wronglyPredicted.groupby('annotated').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabeledAsMatrix(tables):\n",
    "    labeledAs = {}\n",
    "    for iRow, table in tables.iterrows():\n",
    "        for iAnnotation, annotation in enumerate(table['annotations']):\n",
    "            currentLabelDict = labeledAs.get(annotation, {})\n",
    "            predictedAs = table['predictions'][iAnnotation]\n",
    "            currentLabelDict[predictedAs] = currentLabelDict.get(predictedAs, 0) + 1\n",
    "            labeledAs[annotation] = currentLabelDict\n",
    "    return labeledAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledAsMatrix = getLabeledAsMatrix(testTables)\n",
    "labeledAsMatrixPD = pd.DataFrame(labeledAsMatrix.values(), index=labeledAsMatrix.keys())\n",
    "labeledAsMatrixPD.fillna(0, inplace=True)\n",
    "labeledAsMatrixPD\n",
    "# ratioDf = pd.DataFrame(ratios, index=ratioLabels, columns=['Other', 'Header', 'Data']).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More detailed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Table ids of incorrectly predicted tables')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "incorrectPredictedTables[['_id', 'predictions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible reasons for incorrect labeling (1):\n",
    "- background color not taken into account enough -> to less examples where background color indicates Header or to many example where a colored cell is not a Header cell\n",
    "- It's a legend and marked as data while we would label it as 'Other' -> taking into account the occurrence of characters like '=' ':' could help?\n",
    "- merged cell doesn't seem to be a good indicator that cell should be 'Other' instead of 'Data'\n",
    "- group header mistaken as real header\n",
    "- maybe the tables (with many rows) in the test set had no header and therefor the size was more important? (need to check if row/col count is taken as feature)\n",
    "- 'bold' style doesn't indicate if it's a header for sure -> tr/thead feature is more important -> if tr/thead is missing, but cell is bold the row still gets marked as 'Data' instead of 'Header' -> introduce feature accross whole row for bold too"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
