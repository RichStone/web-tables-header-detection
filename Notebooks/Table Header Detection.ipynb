{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Header Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "- Conda\n",
    "- MongoDB instance\n",
    "- PyMongo (will be installed by the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} pymongo\n",
    "# !conda install --yes --prefix {sys.prefix} premailer\n",
    "# pip install premailer\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from cssutils import parseStyle\n",
    "from premailer import Premailer\n",
    "import time\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the seed data into mongoDB\n",
    "- initial dataset [Wikipedia TabEL dataset](http://websail-fe.cs.northwestern.edu/TabEL/)\n",
    "- dataset is lacking of some styling information\n",
    "- we're crawling the wikipedia pages on our own\n",
    "  - that should be feasible since we have to use labeled data only (both for training & testing)\n",
    "  - we're taking the TabEL dataset pageID's as starting point, since we know that there should be at least one relational table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line of the TabEL dataset contains one JSON object representing a single table. However, the JSON objects are not contained within a JSON array. We need to wrap the single tables into an array first before we can parse the file as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapJSONObjectLineIntoTable(inputFilePath, outputFilePath):\n",
    "    inputFile = open(inputFilePath, 'r')\n",
    "    outputFile = open(outputFilePath, 'w')\n",
    "\n",
    "    outputFile.write('[')\n",
    "\n",
    "    previousLine = False\n",
    "    for tableLineJsonObject in inputFile:\n",
    "        if (previousLine):\n",
    "            outputFile.write(previousLine + ',')\n",
    "        previousLine = tableLineJsonObject\n",
    "    if (previousLine):\n",
    "        outputFile.write(previousLine)\n",
    "\n",
    "    outputFile.write(']')\n",
    "\n",
    "    inputFile.close()\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if TabEL dataset has been transformed into an array before. If not, we want to parse it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFilePath = os.path.join('data', 'wikipedia_0_50000.json')\n",
    "outputFilePath = os.path.join('data', 'wikipedia_0_50000_fixed.json')\n",
    "if not os.path.isfile(outputFilePath):\n",
    "    wrapJSONObjectLineIntoTable(inputFilePath, outputFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse JSON Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabEL = pd.read_json(os.path.join('data', 'wikipedia_0_50000_fixed.json'))\n",
    "tabEL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 1000 unique page IDs and fetch the HTML content for it. (Update: We skip selecting only 1000 here, since we want a broader selection of pages/ tables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquePageIDs = pd.DataFrame(tabEL['pgId'].unique(), columns=['pgId'])\n",
    "print('Number of pages: ' + str(uniquePageIDs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickRandomSample():\n",
    "    pageIDSample = uniquePageIDs.sample(n=1000)\n",
    "    pageIDSample.reset_index(inplace=True)\n",
    "    pageIDSample.drop(axis='columns', labels='index', inplace=True)\n",
    "\n",
    "# pickRandomSample()\n",
    "\n",
    "pageIDSample = uniquePageIDs\n",
    "pageIDSample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the wikipedia pages and fetch all occurring tables\n",
    "We use the pageID's from the TabEL dataset and crawl the wikipedia html. One page might include multiple tables. We only extract HTML tables with the class `wikitable`. The style from the CSS file gets parsed into inline style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://en.wikipedia.org'\n",
    "wikipediaCSSFilePath = os.path.join('data', 'wikipedia.css')\n",
    "cssFilePath = os.path.join('data', 'wikipedia.css')\n",
    "cssFile = open(cssFilePath, 'r')\n",
    "css = cssFile.read()\n",
    "instance = Premailer(\n",
    "    base_url=BASE_URL,\n",
    "    exclude_pseudoclasses=False,\n",
    "    include_star_selectors=True,\n",
    "    disable_validation=True,\n",
    "    css_text=css,\n",
    "    allow_network=False,\n",
    "    cssutils_logging_level='CRITICAL'\n",
    ")\n",
    "\n",
    "def crawl(tabEL):\n",
    "    print(tabEL.name)\n",
    "    payload = { 'curid': str(tabEL['pgId']) }\n",
    "    html = requests.get(BASE_URL, params=payload).text\n",
    "    htmlWithInlineCSS = instance.transform(str(html))\n",
    "    return htmlWithInlineCSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "pages = db.pages\n",
    "\n",
    "def batchCrawl():\n",
    "    pageIDSample['HTML'] = pageIDSample.apply(crawl, axis='columns')\n",
    "    pageIDSample['HTML'] = pageIDSample['HTML'].str.replace('\\n', '')\n",
    "    pageIDSample['HTML'] = pageIDSample['HTML'].str.replace('\\t', '')\n",
    "    pages.insert_many(pageIDSample.to_dict('records'))\n",
    "    # pageIDSample.to_json(os.path.join(\"data\", \"crawled.json\"))\n",
    "    client.close()\n",
    "\n",
    "def sequenceCrawl():\n",
    "    for index, row in pageIDSample.iterrows():\n",
    "        inlineHTML = crawl(row)\n",
    "        inlineHTML = inlineHTML.replace('\\n', '')\n",
    "        inlineHTML = inlineHTML.replace('\\t', '')\n",
    "        row['HTML'] = inlineHTML\n",
    "        pages.insert_one(row.to_dict())\n",
    "    client.close()\n",
    "\n",
    "print('Begin time: ' + str(datetime.now()))\n",
    "sequenceCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "pages = db.pages\n",
    "pages.insert_many(pageIDSample.to_dict('records'))\n",
    "# pageIDSample.to_json(os.path.join(\"data\", \"crawled.json\"))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "pages = db.pages\n",
    "cursor = pages.find({})\n",
    "pageIDSample = pd.DataFrame(list(cursor))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageIDSample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the tables along with some metadata. For each row we assign an unique ID (the index of the row within the table) and a tag (whether the row includes `th-tags` only or is contained within a `thead`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADLINE_PATTERN = re.compile('(h|H)\\d')\n",
    "LABEL_CONTROLS = [\n",
    "    {\n",
    "        'label': 'Header',\n",
    "        'color': 'light-blue'\n",
    "    }, {\n",
    "        'label': 'Data',\n",
    "        'color': 'lime'\n",
    "    }, {\n",
    "        'label': 'Other',\n",
    "        'color': 'orange'\n",
    "    }\n",
    "];\n",
    "\n",
    "def extractPageTitle(soup):\n",
    "    headlines = soup.select('h1')\n",
    "    return headlines[0].text if len(headlines) > 0 else 'N/A'\n",
    "\n",
    "def extractTableTitle(table):\n",
    "    for sibling in table.previous_siblings:\n",
    "        if (sibling is not None and sibling.name is not None and HEADLINE_PATTERN.match(sibling.name)):\n",
    "            return sibling.text\n",
    "    return 'N/A'\n",
    "\n",
    "def addLabelControls(row, rowIndex, soup):\n",
    "    labelControlTag = soup.new_tag(\n",
    "        'th',\n",
    "        attrs={\n",
    "            'class': 'flex space-evenly'\n",
    "        }\n",
    "    )\n",
    "    for labelControl in LABEL_CONTROLS:\n",
    "        labelControlButton = soup.new_tag(\n",
    "            'a',\n",
    "            attrs={\n",
    "                'class': 'labelButton waves-effect waves-light btn-small ' + labelControl['color'],\n",
    "                'onClick': 'annotate(' + str(rowIndex) + ', \"' + labelControl['label'] + '\");',\n",
    "            }\n",
    "        )\n",
    "        labelControlButton.string = labelControl['label']\n",
    "        labelControlTag.append(labelControlButton)\n",
    "    row.insert(0, labelControlTag)\n",
    "    \n",
    "def tagRow(row, rowIndex, soup, isHead=False):\n",
    "    row['data-label'] = 'Header' if isHead else 'Data'\n",
    "    row['data-row-index'] = rowIndex\n",
    "    addLabelControls(row, rowIndex, soup)\n",
    "    \n",
    "def isHeaderRow(row):\n",
    "    thTags = row.find_all('th', recursive=False)\n",
    "    childCount = len(row.contents)\n",
    "    return childCount == len(thTags) or row.parent.name == 'thead'\n",
    "\n",
    "def tagRows(table, soup):\n",
    "    rows = table.find_all('tr')\n",
    "    annotations = []\n",
    "    for rowIndex, row in enumerate(rows):\n",
    "        isHeader = isHeaderRow(row)\n",
    "        tagRow(row, rowIndex, soup, isHeader)  \n",
    "        annotations.append('Header' if isHeader else 'Data')\n",
    "    return annotations\n",
    "\n",
    "def removeTableWidthLimitation(table):\n",
    "    if not table.has_attr('style'):\n",
    "        return\n",
    "    tableStyle = parseStyle(table['style'])\n",
    "    tableStyle['width'] = '100%'\n",
    "    tableStyle['font-size'] = '100%'\n",
    "    table['style'] = tableStyle.cssText\n",
    "        \n",
    "def extractTableInformation(table, pageID, tableIndex, pageTitle, soup):\n",
    "    extractedInformation = {\n",
    "        'pageID': pageID,\n",
    "        'tableIndex': tableIndex,\n",
    "        'pageTitle': pageTitle\n",
    "    }\n",
    "    extractedInformation['html'] = table.prettify()\n",
    "    annotations = tagRows(table, soup)\n",
    "    removeTableWidthLimitation(table)\n",
    "    extractedInformation['taggedHtml'] = table.prettify()\n",
    "    extractedInformation['annotations'] = annotations\n",
    "    extractedInformation['tableTitle'] = extractTableTitle(table)\n",
    "    return extractedInformation\n",
    "\n",
    "def hasNestedTable(table):\n",
    "    return len(table.select('table')) > 0\n",
    "\n",
    "def extractTables(page):\n",
    "    soup = BeautifulSoup(page['HTML'])\n",
    "    pageTitle = extractPageTitle(soup)\n",
    "    wikiTables = soup.select('table.wikitable')\n",
    "    extractedTables = []\n",
    "    for tableIndex, table in enumerate(wikiTables):\n",
    "        if hasNestedTable(table):\n",
    "            continue\n",
    "        extractedTable = extractTableInformation(table, page['pgId'], tableIndex, pageTitle, soup)\n",
    "        extractedTables.append(extractedTable)\n",
    "    return extractedTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "for extractedTables in pageIDSample.apply(extractTables, axis='columns').values:\n",
    "    if len(extractedTables) > 0:\n",
    "        tables.insert_many(extractedTables)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can now get labeled using the provided [labeling tool](https://github.com/RichStone/web-tables-header-detection/tree/master/Labeling%20Tool). However, we may use the feature extraction to enhance the table selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({})\n",
    "tables = pd.DataFrame(list(cursor))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_TEXT_THRESHOLD = 20\n",
    "LONG_TEXT_THRESHOLD = 255\n",
    "\n",
    "def isInt(value):\n",
    "    try: \n",
    "        int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def getRowSpan(cell):\n",
    "    if cell.has_attr('rowspan') and isInt(cell['rowspan']):\n",
    "        return int(cell['rowspan'])\n",
    "    return 1\n",
    "    \n",
    "def getColSpan(cell):\n",
    "    if cell.has_attr('colspan') and isInt(cell['colspan']):\n",
    "        return int(cell['colspan'])\n",
    "    return 1\n",
    "\n",
    "def isMerged(cell):\n",
    "    return (\n",
    "        getColSpan(cell) > 1 or\n",
    "        getRowSpan(cell) > 1\n",
    "    )\n",
    "\n",
    "def isCenterAligned(cell, style):\n",
    "    return (\n",
    "        (cell.has_attr('align') and cell['align'] == 'center') or\n",
    "        (style is not None and 'text-align' in style and style['text-align'] == 'center')\n",
    "    )\n",
    "\n",
    "def isThOrInTHead(cell):\n",
    "    row = cell.parent\n",
    "    rowParent = row.parent\n",
    "    return (\n",
    "        cell.name == 'th' or\n",
    "        rowParent.name == 'thead'\n",
    "    )\n",
    "\n",
    "def extractLayoutFeatures(cell, style):\n",
    "    return {\n",
    "        'isMerged': isMerged(cell),\n",
    "        'isCenterAligned': isCenterAligned(cell, style),\n",
    "        'isTHOrInTHead': isThOrInTHead(cell)\n",
    "    }\n",
    "\n",
    "def isBold(cell, style):\n",
    "    return bool(\n",
    "        style is not None and (\n",
    "            style['font-weight'] == 'bold' or \n",
    "            style['font-style'] == 'bold'\n",
    "        ) or\n",
    "        cell.find('b') or\n",
    "        cell.find('strong')\n",
    "    )\n",
    "\n",
    "def isItalic(cell, style):\n",
    "    return bool(cell.find('i'))\n",
    "\n",
    "def isUnderlined(cell, style):\n",
    "    return bool(\n",
    "        cell.find('u') or\n",
    "        style is not None and (\n",
    "            style['text-decoration'] == 'underline' or\n",
    "            style['font-style'] == 'bold'\n",
    "        )\n",
    "    )\n",
    "\n",
    "def isColored(cell, style):\n",
    "    return (\n",
    "        style is not None and (\n",
    "            'background-color' in style or\n",
    "            'color' in style\n",
    "        )\n",
    "    )\n",
    "\n",
    "def extractStyleFeatures(cell, style):\n",
    "    return {\n",
    "        'isBold': isBold(cell, style),\n",
    "        'isItalic': isItalic(cell, style),\n",
    "        'isUnderlined': isUnderlined(cell, style)\n",
    "    }\n",
    "\n",
    "def getCellStyle(cell):\n",
    "    return parseStyle(cell['style']) if cell.has_attr('style') else None\n",
    "\n",
    "def getContentLength(cell):\n",
    "    return len(re.sub('\\s+',' ', cell.get_text()).split())\n",
    "\n",
    "def isEmpty(cell):\n",
    "    return getContentLength(cell) == 0\n",
    "\n",
    "def isText(cell):\n",
    "    return cell.get_text().isalpha()\n",
    "\n",
    "def isNumeric(cell):\n",
    "    return cell.get_text().isdigit()\n",
    "\n",
    "def isDate(cell):\n",
    "    try: \n",
    "        parse(cell.get_text(), fuzzy=False)\n",
    "        return True\n",
    "    except (ValueError, OverflowError):\n",
    "        return False\n",
    "    \n",
    "def isShortText(cell):\n",
    "    return getContentLength(cell) <= SHORT_TEXT_THRESHOLD\n",
    "\n",
    "def isLongText(cell):\n",
    "    return getContentLength(cell) > LONG_TEXT_THRESHOLD\n",
    "\n",
    "def isTotal(cell):\n",
    "    return cell.get_text().lower() == 'total'\n",
    "\n",
    "def extractValueFeatures(cell):\n",
    "    return {\n",
    "        'isEmpty': isEmpty(cell),\n",
    "        'isText': isText(cell),\n",
    "        'isNumeric': isNumeric(cell),\n",
    "        'isDate': isDate(cell),\n",
    "        'isShortText': isShortText(cell),\n",
    "        'isLongText': isLongText(cell),\n",
    "        'isTotal': isTotal(cell)\n",
    "    }\n",
    "\n",
    "def mapDictBoolValuesToInt(dictionary):\n",
    "    return { key: int(value) for key, value in dictionary.items() }\n",
    "\n",
    "def applyColSpanFactor(dictionary, colSpan):\n",
    "    return { key: value * colSpan for key, value in dictionary.items() }\n",
    "\n",
    "def extractCellFeatures(cell, startRowIndex):\n",
    "    cellStyle = getCellStyle(cell)\n",
    "    boolCellFeatures = {\n",
    "        **extractLayoutFeatures(cell, cellStyle),\n",
    "        **extractStyleFeatures(cell, cellStyle),\n",
    "        **extractValueFeatures(cell)\n",
    "    }\n",
    "    intCellFeatures = mapDictBoolValuesToInt(boolCellFeatures)\n",
    "    # intCellFeatures = applyColSpanFactor(intCellFeatures, getColSpan(cell))\n",
    "    cellFeatures = []\n",
    "    rowSpan = getRowSpan(cell)\n",
    "    for rowIndex in range(startRowIndex, startRowIndex + rowSpan):\n",
    "        featureCopy = dict(intCellFeatures)\n",
    "        featureCopy['row'] = rowIndex\n",
    "        featureCopy['colCount'] = 1\n",
    "        cellFeatures.append(featureCopy)\n",
    "    return cellFeatures\n",
    "\n",
    "def merge(featuresA, featuresB):\n",
    "    return { k: featuresA.get(k, 0) + featuresB.get(k, 0) for k in set(featuresA) | set(featuresB) }\n",
    "\n",
    "def stringifyDictKeys(dictionary):\n",
    "    return { str(key): value for key, value in dictionary.items() }\n",
    "\n",
    "def extractFeatures(table):\n",
    "    soup = BeautifulSoup(table['html'])\n",
    "    rows = soup.select('tr')\n",
    "    rowsFeatures = {}\n",
    "    for rowIndex, row in enumerate(rows):\n",
    "        for cell in row.children:\n",
    "            if type(cell) is not Tag:\n",
    "                continue\n",
    "            for rowFeatures in extractCellFeatures(cell, rowIndex):\n",
    "                rowFeatureIndex = rowFeatures.pop('row')\n",
    "                merged = merge(rowsFeatures.get(rowFeatureIndex, {}), rowFeatures)\n",
    "                rowsFeatures[rowFeatureIndex] = merged\n",
    "    rowsFeatures = stringifyDictKeys(rowsFeatures)\n",
    "    return rowsFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables['features'] = tables.apply(extractFeatures, axis='columns')\n",
    "tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tablesCollection = db.tables\n",
    "tablesCollection.insert_many(tables.to_dict('records'))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Tables for Labeling\n",
    "70% of all tables are considered 'simple'. We don't want to spend too much time labeling only simple tables. We wan't to have more interesting tables and are therefor making a thoughtful selection of tables.\n",
    "Goal: 1000 Tables in total, where ideally 250 are randomly selected, 250 have no header, 250 have at least one merged cell and 250 tables that do have bold cells which are not located in the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({})\n",
    "tables = pd.DataFrame(list(cursor))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = tables.loc[lambda tables: tables['features'].apply(lambda features: len(features) > 1)]\n",
    "print('Amount of tables with row count greater 1: ' + str(tables.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasTableAHeader(tableFeatures):\n",
    "    for rowIndex, rowFeatures in tableFeatures.items():  \n",
    "        if (rowFeatures['isTHOrInTHead'] > 0):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "tablesWithNoHeader = tables.loc[lambda tables: tables['features'].apply(lambda features: not hasTableAHeader(features))]\n",
    "print('Amount of tables without a header: ' + str(tablesWithNoHeader.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasBoldStyleOutsideHead(tableFeatures):\n",
    "    for rowIndex, rowFeatures in tableFeatures.items():  \n",
    "        if (rowFeatures['isBold'] > 0 and rowFeatures['isTHOrInTHead'] == 0):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "tablesWithBoldStyleOutsideHead = tables.loc[lambda tables: tables['features'].apply(hasBoldStyleOutsideHead)]\n",
    "print('Amount of tables with bold styles outside of header: ' + str(tablesWithBoldStyleOutsideHead.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasMergedCellsOutsideHead(tableFeatures):\n",
    "    for rowIndex, rowFeatures in tableFeatures.items():  \n",
    "        if (rowFeatures['isMerged'] > 0 and rowFeatures['isTHOrInTHead'] == 0):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "tablesWithMergedCellsOutsideHead = tables.loc[lambda tables: tables['features'].apply(lambda features: hasMergedCellsOutsideHead(features))]\n",
    "print('Amount of tables with merged cells outside of header: ' + str(tablesWithMergedCellsOutsideHead.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTables = tablesWithNoHeader.sample(n=250)\n",
    "\n",
    "reducedTablesWithMergedCellsOutsideHead = tablesWithMergedCellsOutsideHead.loc[~tablesWithMergedCellsOutsideHead.index.isin(list(labelTables.index))]\n",
    "labelTables = pd.concat([labelTables, reducedTablesWithMergedCellsOutsideHead.sample(n=250)])\n",
    "\n",
    "reducedTablesWithBoldStyleOutsideHead = tablesWithBoldStyleOutsideHead.loc[~tablesWithBoldStyleOutsideHead.index.isin(list(labelTables.index))]\n",
    "labelTables = pd.concat([labelTables, reducedTablesWithBoldStyleOutsideHead.sample(n=250)])\n",
    "\n",
    "remainingTables = tables.loc[~tables.index.isin(list(labelTables.index))]\n",
    "labelTables = pd.concat([labelTables, remainingTables.sample(n=250)])\n",
    "labelTables = labelTables.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "labelTables.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTables1 = labelTables.head(500)\n",
    "labelTables2 = labelTables.tail(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "labelTables1Collection = db.labelTables1\n",
    "labelTables1Collection.insert_many(labelTables1.to_dict('records'))\n",
    "labelTables2Collection = db.labelTables2\n",
    "labelTables2Collection.insert_many(labelTables2.to_dict('records'))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({})\n",
    "tables = pd.DataFrame(list(cursor))\n",
    "client.close()\n",
    "tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcA(c, r):\n",
    "    if (c == 0):\n",
    "        return 0\n",
    "    if (c == r):\n",
    "        return r\n",
    "    if (c > r / 2.0):\n",
    "        return math.floor(math.log2(r - c) + 1)\n",
    "    return math.floor(math.log2(c) + 1)\n",
    "\n",
    "def calcB(c, r):\n",
    "    return math.floor(math.log2(r))\n",
    "\n",
    "def isInSameBin(rowA, rowB, featureKey):\n",
    "    return (\n",
    "        calcB(rowA[featureKey], rowA['colCount']) == calcB(rowB[featureKey], rowB['colCount']) and \n",
    "        calcA(rowA[featureKey], rowA['colCount']) == calcA(rowB[featureKey], rowB['colCount'])\n",
    "    )\n",
    "\n",
    "def logBinTable(table):\n",
    "    logBins = {}\n",
    "    for rowIndex, row in table['features'].items():\n",
    "        logBin = dict(row)\n",
    "        colCount = logBin.pop('colCount')\n",
    "        logBin = { \n",
    "            featureKey: { \n",
    "                'a': calcA(feature, colCount),\n",
    "                'b': calcB(feature, colCount)\n",
    "            } for featureKey, feature in logBin.items() \n",
    "        }\n",
    "        logBins[rowIndex] = logBin\n",
    "    return logBins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables['logBin'] = tables.apply(logBinTable, axis='columns')\n",
    "tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tablesCollection = db.tables\n",
    "tablesCollection.insert_many(tables.to_dict('records'))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
