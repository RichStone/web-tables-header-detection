{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Header Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "- Conda or pip\n",
    "- MongoDB instance\n",
    "- PyMongo (will be installed by the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} pymongo\n",
    "#!conda install --yes --prefix {sys.prefix} premailer\n",
    "\n",
    "#!{sys.executable} -m pip install numpy --upgrade\n",
    "#!{sys.executable} -m pip install pandas\n",
    "#!{sys.executable} -m pip install cssutils\n",
    "#!{sys.executable} -m pip install premailer\n",
    "#!{sys.executable} -m pip install python-crfsuite\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from cssutils import parseStyle\n",
    "from premailer import Premailer\n",
    "import time\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pycrfsuite\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the seed data into mongoDB\n",
    "- initial dataset [Wikipedia TabEL dataset](http://websail-fe.cs.northwestern.edu/TabEL/)\n",
    "- dataset is lacking of some styling information\n",
    "- we're crawling the wikipedia pages on our own\n",
    "  - that should be feasible since we have to use labeled data only (both for training & testing)\n",
    "  - we're taking the TabEL dataset pageID's as starting point, since we know that there should be at least one relational table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line of the TabEL dataset contains one JSON object representing a single table. However, the JSON objects are not contained within a JSON array. We need to wrap the single tables into an array first before we can parse the file as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapJSONObjectLineIntoTable(inputFilePath, outputFilePath):\n",
    "    inputFile = open(inputFilePath, 'r')\n",
    "    outputFile = open(outputFilePath, 'w')\n",
    "\n",
    "    outputFile.write('[')\n",
    "\n",
    "    previousLine = False\n",
    "    for tableLineJsonObject in inputFile:\n",
    "        if (previousLine):\n",
    "            outputFile.write(previousLine + ',')\n",
    "        previousLine = tableLineJsonObject\n",
    "    if (previousLine):\n",
    "        outputFile.write(previousLine)\n",
    "\n",
    "    outputFile.write(']')\n",
    "\n",
    "    inputFile.close()\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if TabEL dataset has been transformed into an array before. If not, we want to parse it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFilePath = os.path.join('data', 'wikipedia_0_50000.json')\n",
    "outputFilePath = os.path.join('data', 'wikipedia_0_50000_fixed.json')\n",
    "if not os.path.isfile(outputFilePath):\n",
    "    wrapJSONObjectLineIntoTable(inputFilePath, outputFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse JSON Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabEL = pd.read_json(os.path.join('data', 'wikipedia_0_50000_fixed.json'))\n",
    "tabEL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 1000 unique page IDs and fetch the HTML content for it. (Update: We skip selecting only 1000 here, since we want a broader selection of pages/ tables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquePageIDs = pd.DataFrame(tabEL['pgId'].unique(), columns=['pgId'])\n",
    "print('Number of pages: ' + str(uniquePageIDs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickRandomSample():\n",
    "    pageIDSample = uniquePageIDs.sample(n=1000)\n",
    "    pageIDSample.reset_index(inplace=True)\n",
    "    pageIDSample.drop(axis='columns', labels='index', inplace=True)\n",
    "\n",
    "# pickRandomSample()\n",
    "\n",
    "pageIDSample = uniquePageIDs\n",
    "pageIDSample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the wikipedia pages and fetch all occurring tables\n",
    "We use the pageID's from the TabEL dataset and crawl the wikipedia html. One page might include multiple tables. We only extract HTML tables with the class `wikitable`. The style from the CSS file gets parsed into inline style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://en.wikipedia.org'\n",
    "wikipediaCSSFilePath = os.path.join('data', 'wikipedia.css')\n",
    "cssFilePath = os.path.join('data', 'wikipedia.css')\n",
    "cssFile = open(cssFilePath, 'r')\n",
    "css = cssFile.read()\n",
    "instance = Premailer(\n",
    "    base_url=BASE_URL,\n",
    "    exclude_pseudoclasses=False,\n",
    "    include_star_selectors=True,\n",
    "    disable_validation=True,\n",
    "    css_text=css,\n",
    "    allow_network=False,\n",
    "    cssutils_logging_level='CRITICAL'\n",
    ")\n",
    "\n",
    "def crawl(tabEL):\n",
    "    print(tabEL.name)\n",
    "    payload = { 'curid': str(tabEL['pgId']) }\n",
    "    html = requests.get(BASE_URL, params=payload).text\n",
    "    htmlWithInlineCSS = instance.transform(str(html))\n",
    "    return htmlWithInlineCSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "pages = db.pages\n",
    "\n",
    "def batchCrawl():\n",
    "    pageIDSample['HTML'] = pageIDSample.apply(crawl, axis='columns')\n",
    "    pageIDSample['HTML'] = pageIDSample['HTML'].str.replace('\\n', '')\n",
    "    pageIDSample['HTML'] = pageIDSample['HTML'].str.replace('\\t', '')\n",
    "    pages.insert_many(pageIDSample.to_dict('records'))\n",
    "    # pageIDSample.to_json(os.path.join(\"data\", \"crawled.json\"))\n",
    "    client.close()\n",
    "\n",
    "def sequenceCrawl():\n",
    "    for index, row in pageIDSample.iterrows():\n",
    "        inlineHTML = crawl(row)\n",
    "        inlineHTML = inlineHTML.replace('\\n', '')\n",
    "        inlineHTML = inlineHTML.replace('\\t', '')\n",
    "        row['HTML'] = inlineHTML\n",
    "        pages.insert_one(row.to_dict())\n",
    "    client.close()\n",
    "\n",
    "print('Begin time: ' + str(datetime.now()))\n",
    "sequenceCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "pages = db.pages\n",
    "pages.insert_many(pageIDSample.to_dict('records'))\n",
    "# pageIDSample.to_json(os.path.join(\"data\", \"crawled.json\"))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "pages = db.pages\n",
    "cursor = pages.find({})\n",
    "pageIDSample = pd.DataFrame(list(cursor))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageIDSample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the tables along with some metadata. For each row we assign an unique ID (the index of the row within the table) and a tag (whether the row includes `th-tags` only or is contained within a `thead`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADLINE_PATTERN = re.compile('(h|H)\\d')\n",
    "LABEL_CONTROLS = [\n",
    "    {\n",
    "        'label': 'Header',\n",
    "        'color': 'light-blue'\n",
    "    }, {\n",
    "        'label': 'Data',\n",
    "        'color': 'lime'\n",
    "    }, {\n",
    "        'label': 'Other',\n",
    "        'color': 'orange'\n",
    "    }\n",
    "];\n",
    "\n",
    "def extractPageTitle(soup):\n",
    "    headlines = soup.select('h1')\n",
    "    return headlines[0].text if len(headlines) > 0 else 'N/A'\n",
    "\n",
    "def extractTableTitle(table):\n",
    "    for sibling in table.previous_siblings:\n",
    "        if (sibling is not None and sibling.name is not None and HEADLINE_PATTERN.match(sibling.name)):\n",
    "            return sibling.text\n",
    "    return 'N/A'\n",
    "\n",
    "def addLabelControls(row, rowIndex, soup):\n",
    "    labelControlTag = soup.new_tag(\n",
    "        'th',\n",
    "        attrs={\n",
    "            'class': 'flex space-evenly'\n",
    "        }\n",
    "    )\n",
    "    for labelControl in LABEL_CONTROLS:\n",
    "        labelControlButton = soup.new_tag(\n",
    "            'a',\n",
    "            attrs={\n",
    "                'class': 'labelButton waves-effect waves-light btn-small ' + labelControl['color'],\n",
    "                'onClick': 'annotate(' + str(rowIndex) + ', \"' + labelControl['label'] + '\");',\n",
    "            }\n",
    "        )\n",
    "        labelControlButton.string = labelControl['label']\n",
    "        labelControlTag.append(labelControlButton)\n",
    "    row.insert(0, labelControlTag)\n",
    "    \n",
    "def tagRow(row, rowIndex, soup, isHead=False):\n",
    "    row['data-label'] = 'Header' if isHead else 'Data'\n",
    "    row['data-row-index'] = rowIndex\n",
    "    addLabelControls(row, rowIndex, soup)\n",
    "    \n",
    "def isHeaderRow(row):\n",
    "    thTags = row.find_all('th', recursive=False)\n",
    "    childCount = len(row.contents)\n",
    "    return childCount == len(thTags) or row.parent.name == 'thead'\n",
    "\n",
    "def tagRows(table, soup):\n",
    "    rows = table.find_all('tr')\n",
    "    annotations = []\n",
    "    for rowIndex, row in enumerate(rows):\n",
    "        isHeader = isHeaderRow(row)\n",
    "        tagRow(row, rowIndex, soup, isHeader)  \n",
    "        annotations.append('Header' if isHeader else 'Data')\n",
    "    return annotations\n",
    "\n",
    "def removeTableWidthLimitation(table):\n",
    "    if not table.has_attr('style'):\n",
    "        return\n",
    "    tableStyle = parseStyle(table['style'])\n",
    "    tableStyle['width'] = '100%'\n",
    "    tableStyle['font-size'] = '100%'\n",
    "    table['style'] = tableStyle.cssText\n",
    "        \n",
    "def extractTableInformation(table, pageID, tableIndex, pageTitle, soup):\n",
    "    extractedInformation = {\n",
    "        'pageID': pageID,\n",
    "        'tableIndex': tableIndex,\n",
    "        'pageTitle': pageTitle\n",
    "    }\n",
    "    extractedInformation['html'] = table.prettify()\n",
    "    annotations = tagRows(table, soup)\n",
    "    removeTableWidthLimitation(table)\n",
    "    extractedInformation['taggedHtml'] = table.prettify()\n",
    "    extractedInformation['annotations'] = annotations\n",
    "    extractedInformation['tableTitle'] = extractTableTitle(table)\n",
    "    return extractedInformation\n",
    "\n",
    "def hasNestedTable(table):\n",
    "    return len(table.select('table')) > 0\n",
    "\n",
    "def extractTables(page):\n",
    "    soup = BeautifulSoup(page['HTML'])\n",
    "    pageTitle = extractPageTitle(soup)\n",
    "    wikiTables = soup.select('table.wikitable')\n",
    "    extractedTables = []\n",
    "    for tableIndex, table in enumerate(wikiTables):\n",
    "        if hasNestedTable(table):\n",
    "            continue\n",
    "        extractedTable = extractTableInformation(table, page['pgId'], tableIndex, pageTitle, soup)\n",
    "        extractedTables.append(extractedTable)\n",
    "    return extractedTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "for extractedTables in pageIDSample.apply(extractTables, axis='columns').values:\n",
    "    if len(extractedTables) > 0:\n",
    "        tables.insert_many(extractedTables)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can now get labeled using the provided [labeling tool](https://github.com/RichStone/web-tables-header-detection/tree/master/Labeling%20Tool). However, we may use the feature extraction to enhance the table selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({})\n",
    "tables = pd.DataFrame(list(cursor))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_TEXT_THRESHOLD = 20\n",
    "LONG_TEXT_THRESHOLD = 255\n",
    "\n",
    "def isInt(value):\n",
    "    try: \n",
    "        int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def getRowSpan(cell):\n",
    "    if cell.has_attr('rowspan') and isInt(cell['rowspan']):\n",
    "        return int(cell['rowspan'])\n",
    "    return 1\n",
    "    \n",
    "def getColSpan(cell):\n",
    "    if cell.has_attr('colspan') and isInt(cell['colspan']):\n",
    "        return int(cell['colspan'])\n",
    "    return 1\n",
    "\n",
    "def isMerged(cell):\n",
    "    return (\n",
    "        getColSpan(cell) > 1 or\n",
    "        getRowSpan(cell) > 1\n",
    "    )\n",
    "\n",
    "def isCenterAligned(cell, style):\n",
    "    return (\n",
    "        (cell.has_attr('align') and cell['align'] == 'center') or\n",
    "        (style is not None and 'text-align' in style and style['text-align'] == 'center')\n",
    "    )\n",
    "\n",
    "def isThOrInTHead(cell):\n",
    "    row = cell.parent\n",
    "    rowParent = row.parent\n",
    "    return (\n",
    "        cell.name == 'th' or\n",
    "        rowParent.name == 'thead'\n",
    "    )\n",
    "\n",
    "def extractLayoutFeatures(cell, style):\n",
    "    return {\n",
    "        'hasColSpan': getColSpan(cell) > 1,\n",
    "        'hasRowSpan': getRowSpan(cell) > 1,\n",
    "        'isMerged': isMerged(cell),\n",
    "        'isCenterAligned': isCenterAligned(cell, style),\n",
    "        'isTHOrInTHead': isThOrInTHead(cell)\n",
    "    }\n",
    "\n",
    "def isCompletlyBold(cell, style):\n",
    "    return style is not None and (\n",
    "            style['font-weight'] == 'bold' or \n",
    "            style['font-style'] == 'bold')\n",
    "\n",
    "def isPartiallyBold(cell, style):\n",
    "    return bool(cell.find('b') or\n",
    "        cell.find('strong'))\n",
    "\n",
    "def isBold(cell, style):\n",
    "    return isCompletlyBold(cell, style) or isPartiallyBold(cell, style)\n",
    "\n",
    "def isItalic(cell, style):\n",
    "    return bool(cell.find('i'))\n",
    "\n",
    "def isUnderlined(cell, style):\n",
    "    return bool(\n",
    "        cell.find('u') or\n",
    "        style is not None and (\n",
    "            style['text-decoration'] == 'underline' or\n",
    "            style['font-style'] == 'bold'\n",
    "        )\n",
    "    )\n",
    "\n",
    "def isColored(cell, style):\n",
    "    return (\n",
    "        style is not None and (\n",
    "            'background-color' in style or\n",
    "            'color' in style\n",
    "        )\n",
    "    )\n",
    "\n",
    "def extractStyleFeatures(cell, style):\n",
    "    return {\n",
    "        'isCompletlyBold': isCompletlyBold(cell, style),\n",
    "        'isPartiallyBold': isPartiallyBold(cell, style),\n",
    "        'isBold': isBold(cell, style),\n",
    "        'isItalic': isItalic(cell, style),\n",
    "        'isUnderlined': isUnderlined(cell, style)\n",
    "    }\n",
    "\n",
    "def getCellStyle(cell):\n",
    "    return parseStyle(cell['style']) if cell.has_attr('style') else None\n",
    "\n",
    "def getContentLength(cell):\n",
    "    return len(re.sub('\\s+',' ', cell.get_text()).split())\n",
    "\n",
    "def isEmpty(cell):\n",
    "    return getContentLength(cell) == 0\n",
    "\n",
    "def isText(cell):\n",
    "    return cell.get_text().isalpha()\n",
    "\n",
    "def isNumeric(cell):\n",
    "    return cell.get_text().isdigit()\n",
    "\n",
    "def isDate(cell):\n",
    "    try: \n",
    "        parse(cell.get_text(), fuzzy=False)\n",
    "        return True\n",
    "    except (ValueError, OverflowError):\n",
    "        return False\n",
    "    \n",
    "def isShortText(cell):\n",
    "    return getContentLength(cell) <= SHORT_TEXT_THRESHOLD\n",
    "\n",
    "def isLongText(cell):\n",
    "    return getContentLength(cell) > LONG_TEXT_THRESHOLD\n",
    "\n",
    "def isTotal(cell):\n",
    "    return cell.get_text().lower() == 'total'\n",
    "\n",
    "def extractValueFeatures(cell):\n",
    "    return {\n",
    "        'isEmpty': isEmpty(cell),\n",
    "        'isText': isText(cell),\n",
    "        'isNumeric': isNumeric(cell),\n",
    "        'isDate': isDate(cell),\n",
    "        'isShortText': isShortText(cell),\n",
    "        'isLongText': isLongText(cell),\n",
    "        'isTotal': isTotal(cell)\n",
    "    }\n",
    "\n",
    "def mapDictBoolValuesToInt(dictionary):\n",
    "    return { key: int(value) for key, value in dictionary.items() }\n",
    "\n",
    "def applyColSpanFactor(dictionary, colSpan):\n",
    "    return { key: value * colSpan for key, value in dictionary.items() }\n",
    "\n",
    "def merge(featuresA, featuresB):\n",
    "    return { k: featuresA.get(k, 0) + featuresB.get(k, 0) for k in set(featuresA) | set(featuresB) }\n",
    "\n",
    "def stringifyDictKeys(dictionary):\n",
    "    return { str(key): value for key, value in dictionary.items() }\n",
    "\n",
    "def numNormalisedCols(row):\n",
    "    numCols = 0\n",
    "    for cell in row.children:\n",
    "        if type(cell) is Tag:\n",
    "            numCols += getColSpan(cell)\n",
    "    return numCols\n",
    "            \n",
    "def getSimilarity(feature, cell, neighbour, suffix):\n",
    "    similarity = {}\n",
    "    similarity[feature + 'A' + suffix] = cell[feature] and neighbour[feature]\n",
    "    similarity[feature + 'B' + suffix] = cell[feature] and not neighbour[feature]\n",
    "    return similarity\n",
    "    \n",
    "def extractSimilarityFeatures(cell, neighbour, suffix, featureNames):\n",
    "    similarityFeatures = {}\n",
    "    for feature in featureNames:\n",
    "        similarityFeatures = {\n",
    "            **getSimilarity(feature, cell, neighbour, suffix),\n",
    "            **similarityFeatures\n",
    "        }\n",
    "    return similarityFeatures   \n",
    "    \n",
    "def getRowSimilarityFeatures(normalizedFeatureTable, rowIndex, row):\n",
    "    newRow = []\n",
    "    numRows = len(normalizedFeatureTable)\n",
    "    for cellIndex, cell in enumerate(row):\n",
    "        features = cell\n",
    "        featureNames = [key for key in features]\n",
    "        if rowIndex > 0:\n",
    "            features = {\n",
    "                **extractSimilarityFeatures(cell, normalizedFeatureTable[rowIndex - 1][cellIndex], 'u', featureNames), \n",
    "                **features\n",
    "                }\n",
    "        if rowIndex < numRows - 1:\n",
    "            features = {\n",
    "                **extractSimilarityFeatures(cell, normalizedFeatureTable[rowIndex + 1][cellIndex], 'l', featureNames),\n",
    "                **features\n",
    "            }\n",
    "        intCellFeatures = mapDictBoolValuesToInt(features)\n",
    "        newRow.append(intCellFeatures)\n",
    "        return newRow\n",
    "    \n",
    "def addSimilarityFeatures(normalizedFeatureTable):\n",
    "    normalizedTableWithSimilarity = []\n",
    "    for rowIndex, row in enumerate(normalizedFeatureTable):\n",
    "        newRow = getRowSimilarityFeatures(normalizedFeatureTable, rowIndex, row)\n",
    "        normalizedTableWithSimilarity.append(newRow)        \n",
    "    return normalizedTableWithSimilarity\n",
    "    \n",
    "def cleanOfEmptyCells(table):\n",
    "    lastEmptyCellIndex = len(table[0])\n",
    "    for row in table:\n",
    "        for cellIndex,cell in enumerate(row):\n",
    "            if cell == 'empty cell':\n",
    "                lastEmptyCellIndex = min(lastEmptyCellIndex, cellIndex)\n",
    "    newTable =[]\n",
    "    for row in table:\n",
    "        newTable.append(row[:lastEmptyCellIndex])\n",
    "    return newTable\n",
    "\n",
    "def getBoolCellFeatures(cell):\n",
    "    cellStyle = getCellStyle(cell)\n",
    "    boolCellFeatures = {\n",
    "        **extractLayoutFeatures(cell, cellStyle),\n",
    "        **extractStyleFeatures(cell, cellStyle),\n",
    "        **extractValueFeatures(cell)\n",
    "    }\n",
    "    boolCellFeatures['colCount'] = 1\n",
    "    return boolCellFeatures\n",
    "\n",
    "def fillNormalizedFeatureTable(rows, numRows, numCols):\n",
    "    normalizedTable = [['empty cell' for i in range(numCols)] for j in range(numRows)]\n",
    "    for rowIndex, row in enumerate(rows):\n",
    "        cellIndex = 0\n",
    "        for cell in row.children:\n",
    "            if type(cell) is not Tag:\n",
    "                continue\n",
    "            boolCellFeatures = getBoolCellFeatures(cell)\n",
    "            # normalize\n",
    "            colSpan = getColSpan(cell)\n",
    "            rowSpan = getRowSpan(cell)\n",
    "            # find the position of the cell\n",
    "            while cellIndex < numCols and normalizedTable[rowIndex][cellIndex] != 'empty cell':\n",
    "                cellIndex += 1\n",
    "            for rIndex in range(rowIndex, min(rowIndex + rowSpan, numRows)):\n",
    "                for cIndex in range(cellIndex, min(cellIndex + colSpan, numCols)):\n",
    "                    normalizedTable[rIndex][cIndex] = boolCellFeatures\n",
    "            cellIndex += colSpan\n",
    "    return normalizedTable\n",
    "        \n",
    "def normalizedFeatureTable(table):\n",
    "    soup = BeautifulSoup(table['html'])\n",
    "    rows = soup.select('tr')\n",
    "    # initialize normalized feature table\n",
    "    numRows = len(rows)\n",
    "    numCols = numNormalisedCols(rows[0])\n",
    "    normalizedTable = fillNormalizedFeatureTable(rows, numRows, numCols)\n",
    "    normalizedTable = cleanOfEmptyCells(normalizedTable)\n",
    "    return addSimilarityFeatures(normalizedTable)    \n",
    "            \n",
    "def extractFeatures(table):\n",
    "    featureTable = normalizedFeatureTable(table)\n",
    "    rowFeatureTable = {}\n",
    "    for rowIndex, row in enumerate(featureTable):\n",
    "        # count how often every feature is true in a row\n",
    "        rowFeatures = {}\n",
    "        for cellFeatures in row:\n",
    "            rowFeatures = merge(rowFeatures, cellFeatures)\n",
    "        rowFeatureTable[rowIndex] = rowFeatures\n",
    "    rowFeatureTable = stringifyDictKeys(rowFeatureTable)\n",
    "    return rowFeatureTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables['features'] = tables.apply(extractFeatures, axis='columns')\n",
    "tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tablesCollection = db.tables\n",
    "dictTables = tables.to_dict('records')\n",
    "for table in dictTables:\n",
    "    tablesCollection.replace_one({'_id': table['_id']}, table, True)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Tables for Labeling\n",
    "70% of all tables are considered 'simple'. We don't want to spend too much time labeling only simple tables. We wan't to have more interesting tables and are therefor making a thoughtful selection of tables.\n",
    "Goal: 1000 Tables in total, where ideally 250 are randomly selected, 250 have no header, 250 have at least one merged cell and 250 tables that do have bold cells which are not located in the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({})\n",
    "tables = pd.DataFrame(list(cursor))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = tables.loc[lambda tables: tables['features'].apply(lambda features: len(features) > 1)]\n",
    "print('Amount of tables with row count greater 1: ' + str(tables.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasTableAHeader(tableFeatures):\n",
    "    for rowIndex, rowFeatures in tableFeatures.items():  \n",
    "        if (rowFeatures['isTHOrInTHead'] > 0):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "tablesWithNoHeader = tables.loc[lambda tables: tables['features'].apply(lambda features: not hasTableAHeader(features))]\n",
    "print('Amount of tables without a header: ' + str(tablesWithNoHeader.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasBoldStyleOutsideHead(tableFeatures):\n",
    "    for rowIndex, rowFeatures in tableFeatures.items():  \n",
    "        if (rowFeatures['isBold'] > 0 and rowFeatures['isTHOrInTHead'] == 0):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "tablesWithBoldStyleOutsideHead = tables.loc[lambda tables: tables['features'].apply(hasBoldStyleOutsideHead)]\n",
    "print('Amount of tables with bold styles outside of header: ' + str(tablesWithBoldStyleOutsideHead.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasMergedCellsOutsideHead(tableFeatures):\n",
    "    for rowIndex, rowFeatures in tableFeatures.items():  \n",
    "        if (rowFeatures['isMerged'] > 0 and rowFeatures['isTHOrInTHead'] == 0):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "tablesWithMergedCellsOutsideHead = tables.loc[lambda tables: tables['features'].apply(lambda features: hasMergedCellsOutsideHead(features))]\n",
    "print('Amount of tables with merged cells outside of header: ' + str(tablesWithMergedCellsOutsideHead.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTables = tablesWithNoHeader.sample(n=250)\n",
    "\n",
    "reducedTablesWithMergedCellsOutsideHead = tablesWithMergedCellsOutsideHead.loc[~tablesWithMergedCellsOutsideHead.index.isin(list(labelTables.index))]\n",
    "labelTables = pd.concat([labelTables, reducedTablesWithMergedCellsOutsideHead.sample(n=250)])\n",
    "\n",
    "reducedTablesWithBoldStyleOutsideHead = tablesWithBoldStyleOutsideHead.loc[~tablesWithBoldStyleOutsideHead.index.isin(list(labelTables.index))]\n",
    "labelTables = pd.concat([labelTables, reducedTablesWithBoldStyleOutsideHead.sample(n=250)])\n",
    "\n",
    "remainingTables = tables.loc[~tables.index.isin(list(labelTables.index))]\n",
    "labelTables = pd.concat([labelTables, remainingTables.sample(n=250)])\n",
    "labelTables = labelTables.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "labelTables.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTables1 = labelTables.head(500)\n",
    "labelTables2 = labelTables.tail(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "labelTables1Collection = db.labelTables1\n",
    "labelTables1Collection.insert_many(labelTables1.to_dict('records'))\n",
    "labelTables2Collection = db.labelTables2\n",
    "labelTables2Collection.insert_many(labelTables2.to_dict('records'))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({})\n",
    "tables = pd.DataFrame(list(cursor))\n",
    "client.close()\n",
    "tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcA(c, r):\n",
    "    if (c == 0):\n",
    "        return 0\n",
    "    if (c == r):\n",
    "        return r\n",
    "    if (c > r / 2.0):\n",
    "        return math.floor(math.log2(r - c) + 1)\n",
    "    return math.floor(math.log2(c) + 1)\n",
    "\n",
    "def calcB(c, r):\n",
    "    return math.floor(math.log2(r))\n",
    "\n",
    "def isInSameBin(rowA, rowB, featureKey):\n",
    "    return (\n",
    "        calcB(rowA[featureKey], rowA['colCount']) == calcB(rowB[featureKey], rowB['colCount']) and \n",
    "        calcA(rowA[featureKey], rowA['colCount']) == calcA(rowB[featureKey], rowB['colCount'])\n",
    "    )\n",
    "\n",
    "def logBinTable(table):\n",
    "    if len(table['features']) == 0:\n",
    "        return []\n",
    "    logBins = {}\n",
    "    for rowIndex, row in table['features'].items():\n",
    "        logBin = dict(row)\n",
    "        colCount = logBin.pop('colCount')\n",
    "        logBin = { \n",
    "            featureKey: { \n",
    "                'a': calcA(feature, colCount),\n",
    "                'b': calcB(feature, colCount)\n",
    "            } for featureKey, feature in logBin.items() \n",
    "        }\n",
    "        logBins[rowIndex] = logBin\n",
    "    return logBins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables['logBin'] = tables.apply(logBinTable, axis='columns')\n",
    "tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tablesCollection = db.tables\n",
    "dictTables = tables.to_dict('records')\n",
    "for table in dictTables:\n",
    "    tablesCollection.replace_one({'_id': table['_id']}, table, True)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Random Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled tables: 975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>annotatedAt</th>\n",
       "      <th>annotations</th>\n",
       "      <th>features</th>\n",
       "      <th>html</th>\n",
       "      <th>logBin</th>\n",
       "      <th>pageID</th>\n",
       "      <th>pageTitle</th>\n",
       "      <th>predictions</th>\n",
       "      <th>skipped</th>\n",
       "      <th>tableIndex</th>\n",
       "      <th>tableTitle</th>\n",
       "      <th>taggedHtml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d020c18e7ee03eab39ab0ee</td>\n",
       "      <td>1.560430e+12</td>\n",
       "      <td>[Header, Data, Data, Data]</td>\n",
       "      <td>{'0': {'isDateBl': 0, 'isMerged': 0, 'isItalic...</td>\n",
       "      <td>&lt;table align=\"center\" bgcolor=\"#f8f9fa\" class=...</td>\n",
       "      <td>{'0': {'isDateBl': {'a': 0, 'b': 1}, 'isMerged...</td>\n",
       "      <td>1027342.0</td>\n",
       "      <td>1996 CONCACAF Gold Cup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Venues[edit]</td>\n",
       "      <td>&lt;table align=\"center\" bgcolor=\"#f8f9fa\" class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d020c18e7ee03eab39ab0fa</td>\n",
       "      <td>1.560430e+12</td>\n",
       "      <td>[Header, Data, Data, Data]</td>\n",
       "      <td>{'0': {'isDateBl': 0, 'isMerged': 0, 'isItalic...</td>\n",
       "      <td>&lt;table align=\"center\" bgcolor=\"#f8f9fa\" class=...</td>\n",
       "      <td>{'0': {'isDateBl': {'a': 0, 'b': 3}, 'isMerged...</td>\n",
       "      <td>1027385.0</td>\n",
       "      <td>1998 CONCACAF Gold Cup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Group C[edit]</td>\n",
       "      <td>&lt;table align=\"center\" bgcolor=\"#f8f9fa\" class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d020c18e7ee03eab39ab111</td>\n",
       "      <td>1.560430e+12</td>\n",
       "      <td>[Data, Data, Data, Data, Data, Data, Data]</td>\n",
       "      <td>{'0': {'isDateBl': 0, 'isMerged': 0, 'isItalic...</td>\n",
       "      <td>&lt;table bgcolor=\"#f8f9fa\" class=\"wikitable\" sty...</td>\n",
       "      <td>{'0': {'isDateBl': {'a': 0, 'b': 0}, 'isMerged...</td>\n",
       "      <td>1027435.0</td>\n",
       "      <td>Tomb of the Unknown Soldier (Warsaw)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Battles currently featured on the stone tablet...</td>\n",
       "      <td>&lt;table bgcolor=\"#f8f9fa\" class=\"wikitable\" sty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d020c18e7ee03eab39ab112</td>\n",
       "      <td>1.560430e+12</td>\n",
       "      <td>[Header, Data, Other, Data, Other, Data, Other...</td>\n",
       "      <td>{'0': {'isDateBl': 0, 'isMerged': 0, 'isItalic...</td>\n",
       "      <td>&lt;table bgcolor=\"#f8f9fa\" class=\"wikitable plai...</td>\n",
       "      <td>{'0': {'isDateBl': {'a': 0, 'b': 1}, 'isMerged...</td>\n",
       "      <td>1027443.0</td>\n",
       "      <td>Groovie Goolies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Episodes[edit]</td>\n",
       "      <td>&lt;table bgcolor=\"#f8f9fa\" class=\"wikitable plai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d020c18e7ee03eab39ab11b</td>\n",
       "      <td>1.560430e+12</td>\n",
       "      <td>[Header, Data, Other]</td>\n",
       "      <td>{'0': {'isDateBl': 0, 'isMerged': 0, 'isItalic...</td>\n",
       "      <td>&lt;table bgcolor=\"#f8f9fa\" class=\"wikitable plai...</td>\n",
       "      <td>{'0': {'isDateBl': {'a': 0, 'b': 1}, 'isMerged...</td>\n",
       "      <td>1027471.0</td>\n",
       "      <td>List of The Flintstones episodes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Pilot (1959)[edit]</td>\n",
       "      <td>&lt;table bgcolor=\"#f8f9fa\" class=\"wikitable plai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id   annotatedAt  \\\n",
       "0  5d020c18e7ee03eab39ab0ee  1.560430e+12   \n",
       "1  5d020c18e7ee03eab39ab0fa  1.560430e+12   \n",
       "2  5d020c18e7ee03eab39ab111  1.560430e+12   \n",
       "3  5d020c18e7ee03eab39ab112  1.560430e+12   \n",
       "4  5d020c18e7ee03eab39ab11b  1.560430e+12   \n",
       "\n",
       "                                         annotations  \\\n",
       "0                         [Header, Data, Data, Data]   \n",
       "1                         [Header, Data, Data, Data]   \n",
       "2         [Data, Data, Data, Data, Data, Data, Data]   \n",
       "3  [Header, Data, Other, Data, Other, Data, Other...   \n",
       "4                              [Header, Data, Other]   \n",
       "\n",
       "                                            features  \\\n",
       "0  {'0': {'isDateBl': 0, 'isMerged': 0, 'isItalic...   \n",
       "1  {'0': {'isDateBl': 0, 'isMerged': 0, 'isItalic...   \n",
       "2  {'0': {'isDateBl': 0, 'isMerged': 0, 'isItalic...   \n",
       "3  {'0': {'isDateBl': 0, 'isMerged': 0, 'isItalic...   \n",
       "4  {'0': {'isDateBl': 0, 'isMerged': 0, 'isItalic...   \n",
       "\n",
       "                                                html  \\\n",
       "0  <table align=\"center\" bgcolor=\"#f8f9fa\" class=...   \n",
       "1  <table align=\"center\" bgcolor=\"#f8f9fa\" class=...   \n",
       "2  <table bgcolor=\"#f8f9fa\" class=\"wikitable\" sty...   \n",
       "3  <table bgcolor=\"#f8f9fa\" class=\"wikitable plai...   \n",
       "4  <table bgcolor=\"#f8f9fa\" class=\"wikitable plai...   \n",
       "\n",
       "                                              logBin     pageID  \\\n",
       "0  {'0': {'isDateBl': {'a': 0, 'b': 1}, 'isMerged...  1027342.0   \n",
       "1  {'0': {'isDateBl': {'a': 0, 'b': 3}, 'isMerged...  1027385.0   \n",
       "2  {'0': {'isDateBl': {'a': 0, 'b': 0}, 'isMerged...  1027435.0   \n",
       "3  {'0': {'isDateBl': {'a': 0, 'b': 1}, 'isMerged...  1027443.0   \n",
       "4  {'0': {'isDateBl': {'a': 0, 'b': 1}, 'isMerged...  1027471.0   \n",
       "\n",
       "                              pageTitle predictions  skipped  tableIndex  \\\n",
       "0                1996 CONCACAF Gold Cup         NaN      NaN         1.0   \n",
       "1                1998 CONCACAF Gold Cup         NaN      NaN         4.0   \n",
       "2  Tomb of the Unknown Soldier (Warsaw)         NaN      NaN         0.0   \n",
       "3                       Groovie Goolies         NaN      NaN         0.0   \n",
       "4      List of The Flintstones episodes         NaN      NaN         1.0   \n",
       "\n",
       "                                          tableTitle  \\\n",
       "0                                       Venues[edit]   \n",
       "1                                      Group C[edit]   \n",
       "2  Battles currently featured on the stone tablet...   \n",
       "3                                     Episodes[edit]   \n",
       "4                                 Pilot (1959)[edit]   \n",
       "\n",
       "                                          taggedHtml  \n",
       "0  <table align=\"center\" bgcolor=\"#f8f9fa\" class=...  \n",
       "1  <table align=\"center\" bgcolor=\"#f8f9fa\" class=...  \n",
       "2  <table bgcolor=\"#f8f9fa\" class=\"wikitable\" sty...  \n",
       "3  <table bgcolor=\"#f8f9fa\" class=\"wikitable plai...  \n",
       "4  <table bgcolor=\"#f8f9fa\" class=\"wikitable plai...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({\"annotatedAt\" : {\"$exists\" : True}, \"skipped\": {\"$ne\": True}})\n",
    "tables = pd.DataFrame(list(cursor))\n",
    "client.close()\n",
    "print('Number of labeled tables: ' + str(len(tables)))\n",
    "tables.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split labled data into training and test set using ratio 70/30 and store the ids of the tables of each set afterwards to make different test runs comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of training tables: 682\n",
      "Amount of testing tables: 293\n"
     ]
    }
   ],
   "source": [
    "tableIDs = tables['_id'].astype(str)\n",
    "trainTables, testTables = train_test_split(tableIDs, test_size=0.3)\n",
    "print('Amount of training tables: ' + str(len(trainTables)))\n",
    "print('Amount of testing tables: ' + str(len(testTables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTables.to_json(os.path.join('..', 'data', 'train.json'))\n",
    "testTables.to_json(os.path.join('..', 'data', 'test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLogBin(tables):\n",
    "    return [list(binDictionary.values()) for binDictionary in list(tables['logBin'].values)]\n",
    "\n",
    "def getAnnotations(tables):\n",
    "    return list(tables['annotations'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add normalized rowIndex to logBin features for better comparison to random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRowIndex(tableFeatures):\n",
    "    for table in tableFeatures:\n",
    "        for rowIndex, row in enumerate(table):\n",
    "            row['normalizedRowIndex'] = rowIndex / len(table)\n",
    "    return tableFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTables = pd.DataFrame(json.load(open(os.path.join('..', 'data', 'train.json'), 'r')).values(), columns=['_id'])\n",
    "testTables = pd.DataFrame(json.load(open(os.path.join('..', 'data', 'test.json'), 'r')).values(), columns=['_id'])\n",
    "trainTables = trainTables['_id'].apply(lambda tableID: tables.loc[tables['_id'].astype(str) == tableID].iloc[0])\n",
    "testTables = testTables['_id'].apply(lambda tableID: tables.loc[tables['_id'].astype(str) == tableID].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = getLogBin(trainTables)\n",
    "trainLabels = getAnnotations(trainTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = addRowIndex(trainFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for features, labels in zip(trainFeatures, trainLabels):\n",
    "    trainer.append(features, labels)\n",
    "\n",
    "# params copied from https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.train('../data/everything.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatures = getLogBin(testTables)\n",
    "testLabels = getAnnotations(testTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatures = addRowIndex(testFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('../data/everything.crfsuite')\n",
    "predictions = [tagger.tag(features) for features in testFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = db.tables\n",
    "tables.find\n",
    "for tableId, prediction in zip(list(testTables['_id'].values), predictions):\n",
    "    tables.update_one({'_id': tableId}, {'$set': {'predictions': prediction}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Header       0.93      0.95      0.94       329\n",
      "        Data       0.99      0.99      0.99      4523\n",
      "       Other       0.82      0.75      0.78       237\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      5089\n",
      "   macro avg       0.91      0.90      0.90      5089\n",
      "weighted avg       0.98      0.98      0.98      5089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a mapping of labels to indices\n",
    "labels = {\"Header\": 0, \"Data\": 1, \"Other\": 2}\n",
    "\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[tag] for row in predictions for tag in row])\n",
    "truths = np.array([labels[tag] for row in testLabels for tag in row])\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    target_names=[\"Header\", \"Data\", \"Other\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of rows per cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({\"annotatedAt\" : {\"$exists\" : True}, \"skipped\": {\"$ne\": True}})\n",
    "tables = pd.DataFrame(list(cursor))\n",
    "client.close()\n",
    "tables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalCounter = Counter([item for sublist in list(tables['annotations'].values) for item in sublist])\n",
    "trainCounter = Counter([item for sublist in list(trainTables['annotations'].values) for item in sublist])\n",
    "testCounter = Counter([item for sublist in list(testTables['annotations'].values) for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRatio(counter):\n",
    "    total = sum(list(counter.values()))\n",
    "    for annotation in counter:\n",
    "        print(annotation + ': ' + str(counter[annotation] * 100.0 / total))\n",
    "    print()\n",
    "    \n",
    "print('Total')\n",
    "getRatio(totalCounter)\n",
    "print('Train')\n",
    "getRatio(trainCounter)\n",
    "print('Test')\n",
    "getRatio(testCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({\"predictions\" : {\"$exists\" : True}})\n",
    "predictedTables = pd.DataFrame(list(cursor))\n",
    "client.close()\n",
    "predictedTables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWronglyPredicted(table):\n",
    "    incorrect = []\n",
    "    for rowIndex, label in enumerate(table['annotations']):\n",
    "        if(label != table['predictions'][rowIndex]):\n",
    "            incorrect.append({\n",
    "                'annotated': label,\n",
    "                'predicted': table['predictions'][rowIndex]\n",
    "            })\n",
    "    return incorrect\n",
    "\n",
    "def getCorrectlyPredictedCount(table):\n",
    "    return len(table['annotations']) - table['predictedIncorrectlyCount']\n",
    "    \n",
    "def isWholeTableCorrectlyPredicted(table):\n",
    "    return table['predictedCorrectlyCount'] == len(table['annotations'])\n",
    "\n",
    "def getIncorrectlyPredictedCount(table):\n",
    "    return len(table['wronglyPredicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedTables['wronglyPredicted'] = predictedTables.apply(getWronglyPredicted, axis='columns')\n",
    "predictedTables['predictedIncorrectlyCount'] = predictedTables.apply(getIncorrectlyPredictedCount, axis='columns')\n",
    "predictedTables['predictedCorrectlyCount'] = predictedTables.apply(getCorrectlyPredictedCount, axis='columns')\n",
    "predictedTables['predictedCorrectly'] = predictedTables.apply(isWholeTableCorrectlyPredicted, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctlyPredictedTables = predictedTables.loc[predictedTables['predictedCorrectly']].shape[0]\n",
    "print('Correctly predicted table count: ' + str(correctlyPredictedTables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectPredictedTables = predictedTables.loc[~predictedTables['predictedCorrectly']]\n",
    "incorrectPredictedTables.reset_index(inplace=True)\n",
    "incorrectPredictedTableCount = incorrectPredictedTables.shape[0]\n",
    "print('Incorrect predicted table count: ' + str(incorrectPredictedTableCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelBars(plot):\n",
    "    for p in plot.patches:\n",
    "        plot.annotate(\n",
    "            np.round(p.get_height(), decimals=2),\n",
    "            (\n",
    "                p.get_x() + p.get_width() / 2.,\n",
    "                p.get_height()\n",
    "            ),\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            xytext=(0, 10),\n",
    "            textcoords='offset points'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfIncorrectRowsPerTablePlot = incorrectPredictedTables['predictedIncorrectlyCount'].value_counts().plot(\n",
    "    kind='bar', \n",
    "    figsize=(20, 10)\n",
    ")\n",
    "labelBars(numberOfIncorrectRowsPerTablePlot)\n",
    "numberOfIncorrectRowsPerTablePlot.set(\n",
    "   xlabel='Amount of incorrectly labeled rows per table',\n",
    "    ylabel='Amount of tables',\n",
    "    title='Incorrectly labled rows per table'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRowCount(table):\n",
    "    return len(table['annotations'])\n",
    "\n",
    "rowCountPerTablePlot = incorrectPredictedTables.apply(getRowCount, axis='columns').value_counts().plot(\n",
    "    kind='bar', \n",
    "    figsize=(20, 10)\n",
    ")\n",
    "labelBars(rowCountPerTablePlot)\n",
    "rowCountPerTablePlot.set(\n",
    "   xlabel='Table size (total table row count)',\n",
    "    ylabel='Amount of tables',\n",
    "    title='Dependency between table size (total row count) and prediction correctness'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wronglyPredicted = list(incorrectPredictedTables['wronglyPredicted'])\n",
    "wronglyPredicted = [item for sublist in wronglyPredicted for item in sublist]\n",
    "wronglyPredicted = pd.DataFrame(wronglyPredicted)\n",
    "print('Count of which row type got predicted incorrectly:')\n",
    "wronglyPredicted.groupby('annotated').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Table ids of incorrectly predicted tables')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "incorrectPredictedTables[['_id', 'predictions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible reasons for incorrect labeling (1):\n",
    "- background color not taken into account enough -> to less examples where background color indicates Header or to many example where a colored cell is not a Header cell\n",
    "- It's a legend and marked as data while we would label it as 'Other' -> taking into account the occurrence of characters like '=' ':' could help?\n",
    "- merged cell doesn't seem to be a good indicator that cell should be 'Other' instead of 'Data'\n",
    "- group header mistaken as real header\n",
    "- maybe the tables (with many rows) in the test set had no header and therefor the size was more important? (need to check if row/col count is taken as feature)\n",
    "- 'bold' style doesn't indicate if it's a header for sure -> tr/thead feature is more important -> if tr/thead is missing, but cell is bold the row still gets marked as 'Data' instead of 'Header' -> introduce feature accross whole row for bold too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.bob\n",
    "tables = db.tables\n",
    "cursor = tables.find({\"annotatedAt\" : {\"$exists\" : True}, \"skipped\": {\"$ne\": True}})\n",
    "tables = pd.DataFrame(list(cursor))\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTables = pd.DataFrame(json.load(open(os.path.join('..', 'data', 'train.json'), 'r')).values(), columns=['_id'])\n",
    "testTables = pd.DataFrame(json.load(open(os.path.join('..', 'data', 'test.json'), 'r')).values(), columns=['_id'])\n",
    "trainTables = trainTables['_id'].apply(lambda tableID: tables.loc[tables['_id'].astype(str) == tableID].iloc[0])\n",
    "testTables = testTables['_id'].apply(lambda tableID: tables.loc[tables['_id'].astype(str) == tableID].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forests expect float values so logarithmic binning doesn't work\n",
    "def normalizeRow(rowDict):\n",
    "    colCount = rowDict['colCount']\n",
    "    rowDict.pop('colCount')\n",
    "    for feature in rowDict:\n",
    "        rowDict[feature] = rowDict[feature]/colCount\n",
    "    return rowDict\n",
    " \n",
    "def flattenRowFeatures(tables):\n",
    "    tableFeatures = [tableFeatures for tableFeatures in tables[\"features\"]]\n",
    "    rowFeatures = []\n",
    "    # reduce features to one table with features per row\n",
    "    for table in tableFeatures:  \n",
    "        for rowKey in table:\n",
    "            rowDict = table[rowKey]\n",
    "            rowDict = normalizeRow(rowDict)\n",
    "            rowDict['normalizedRowIndex'] = int(rowKey)/len(table)\n",
    "            rowFeatures.append(rowDict)\n",
    "\n",
    "    rowFeatures = pd.DataFrame(rowFeatures)\n",
    "    return rowFeatures\n",
    "    \n",
    "def removeSimilarityFeatures(rowFeatures):\n",
    "    # clean of features comparing neighbouring rows (because they are NaN in first and last row)\n",
    "    columnsToDrop = []\n",
    "    for columnName in rowFeatures:\n",
    "        if columnName[-2:] == \"Al\" or columnName[-2:] == \"Au\" or columnName[-2:] == \"Bl\" or columnName[-2:] == \"Bu\":\n",
    "            columnsToDrop.append(columnName)\n",
    "    rowFeatures = rowFeatures.drop(columns = columnsToDrop)\n",
    "    return rowFeatures\n",
    "\n",
    "def cleanNanFeatures(rowFeatures):\n",
    "    for column in rowFeatures:\n",
    "        for value in column:\n",
    "            if value == \"NaN\":\n",
    "                value = -1\n",
    "    return rowFeatures\n",
    "    \n",
    "def getRandomForestFeatures(tables):\n",
    "    rowFeatures = flattenRowFeatures(tables)\n",
    "    rowFeatures = cleanNaNFeatures(rowFeatures)\n",
    "    return rowFeatures\n",
    "\n",
    "def flattenAnnotations(tables):\n",
    "    tableLables = [tableAnnotations for tableAnnotations in tables[\"annotations\"]]\n",
    "    rowLables = reduce(list.__add__, tableLables)\n",
    "    labels = {\"Header\": 0, \"Data\": 1, \"Other\": 2}\n",
    "    rowLables = [labels[lable] for lable in rowLables]\n",
    "    return rowLables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = getRandomForestFeatures(trainTables)\n",
    "X_test = getRandomForestFeatures(testTables)\n",
    "y_train = flattenAnnotations(trainTables)\n",
    "y_test = flattenAnnotations(testTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_test)\n",
    "absolutePredictions = [round(p) for p in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Header       0.95      0.96      0.95       318\n",
      "        Data       0.99      0.98      0.99      3718\n",
      "       Other       0.78      0.86      0.82       256\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      4292\n",
      "   macro avg       0.91      0.93      0.92      4292\n",
      "weighted avg       0.97      0.97      0.97      4292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    y_test, absolutePredictions,\n",
    "    target_names=[\"Header\", \"Data\", \"Other\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizedRowIndex   Importance: 0.5\n",
      "isMerged             Importance: 0.13\n",
      "isTHOrInTHead        Importance: 0.12\n",
      "isDate               Importance: 0.09\n",
      "isShortText          Importance: 0.09\n",
      "isBold               Importance: 0.03\n",
      "isCenterAligned      Importance: 0.02\n",
      "isEmpty              Importance: 0.01\n",
      "isItalic             Importance: 0.01\n",
      "isLongText           Importance: 0.0\n",
      "isNumeric            Importance: 0.0\n",
      "isText               Importance: 0.0\n",
      "isTotal              Importance: 0.0\n",
      "isUnderlined         Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "feature_list = list(X_train.columns)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('{:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
